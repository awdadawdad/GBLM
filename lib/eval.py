# Import necessary modules
import time
import torch
import torch.nn as nn

# Import get_loaders function from data module within the same directory
from .data import get_loaders 

# Function to evaluate perplexity (ppl) on a specified model and tokenizer
def eval_ppl(model, tokenizer, device):
    # Set dataset
    dataset = "wikitext2"

    # Print status
    print(f"evaluating on {dataset}")

    # Get the test loader
    _, testloader = get_loaders(
        dataset, seed=0, seqlen=model.seqlen, tokenizer=tokenizer 
    )

    # Evaluate ppl in no grad context to avoid updating the model
    with torch.no_grad():
        ppl = eval_ppl_wikitext(model, testloader, 1, device)
    return ppl 

# Function to evaluate perplexity (ppl) specifically on the wikitext dataset
@torch.no_grad()
def eval_ppl_wikitext(model, testenc, bs=1, device=None):
    # Get model's device directly to avoid device mismatch
    device = next(model.parameters()).device
    seqlen = model.seqlen
    testenc = testenc.input_ids
    nsamples = testenc.numel() // seqlen

    # List to store negative log likelihoods
    nlls = []
    print(f"nsamples {nsamples}")

    # Loop through each batch
    for i in range(0,nsamples,bs):
        if i % 50 == 0:
            print(f"sample {i}")

        # Calculate end index
        j = min(i+bs, nsamples)

        # Prepare inputs and move to device
        batch_size = min(bs, nsamples - i)
        
        inputs = testenc[:, (i * seqlen):((i + batch_size) * seqlen)].to(device)
        lm_logits = model(inputs).logits
        # Cast to fp32 for numerical stability during softmax & cross-entropy
        shift_logits = lm_logits[:, :-1, :].contiguous().float()
        shift_labels = testenc[:, (i * seqlen + 1):((i + batch_size) * seqlen + 1)][:,:shift_logits.shape[1]].to(device)

        # Compute loss
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))

        # Skip NaN or Inf losses to avoid corrupting PPL
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"warning: encountered NaN/Inf loss at sample {i}")
            continue

        # Calculate negative log likelihood
        neg_log_likelihood = loss.float() * seqlen * (j-i)

        # Append to list of negative log likelihoods
        nlls.append(neg_log_likelihood)

    if len(nlls) == 0:
        print("Warning: All samples produced NaN/Inf loss. Model may be severely damaged.")
        return float('inf')

    # Compute perplexity
    ppl = torch.exp(torch.stack(nlls).sum() / (len(nlls) * seqlen))

    # Empty CUDA cache to save memory
    torch.cuda.empty_cache()

    return ppl.item()